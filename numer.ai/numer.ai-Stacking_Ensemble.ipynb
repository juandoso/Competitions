{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling with Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Stacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model. Often times the stacked model (also called 2nd-level model) will outperform each of the individual models due its smoothing nature and ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different. \n",
    "\n",
    "http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "workdir = \"/home/ubuntu/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(workdir+'numerai_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'feature1', u'feature2', u'feature3', u'feature4', u'feature5',\n",
       "       u'feature6', u'feature7', u'feature8', u'feature9', u'feature10',\n",
       "       u'feature11', u'feature12', u'feature13', u'feature14', u'feature15',\n",
       "       u'feature16', u'feature17', u'feature18', u'feature19', u'feature20',\n",
       "       u'feature21', u'feature22', u'feature23', u'feature24', u'feature25',\n",
       "       u'feature26', u'feature27', u'feature28', u'feature29', u'feature30',\n",
       "       u'feature31', u'feature32', u'feature33', u'feature34', u'feature35',\n",
       "       u'feature36', u'feature37', u'feature38', u'feature39', u'feature40',\n",
       "       u'feature41', u'feature42', u'feature43', u'feature44', u'feature45',\n",
       "       u'feature46', u'feature47', u'feature48', u'feature49', u'feature50',\n",
       "       u'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = {\n",
    "    \"folds\" : \"fold\", \n",
    "    \"target\" : \"target\", \n",
    "    \"ids\" : \"t_id\", \n",
    "    \"features\" : [u'feature1', u'feature2', u'feature3', u'feature4', u'feature5',\n",
    "           u'feature6', u'feature7', u'feature8', u'feature9', u'feature10',\n",
    "           u'feature11', u'feature12', u'feature13', u'feature14', u'feature15',\n",
    "           u'feature16', u'feature17', u'feature18', u'feature19', u'feature20',\n",
    "           u'feature21', u'feature22', u'feature23', u'feature24', u'feature25',\n",
    "           u'feature26', u'feature27', u'feature28', u'feature29', u'feature30',\n",
    "           u'feature31', u'feature32', u'feature33', u'feature34', u'feature35',\n",
    "           u'feature36', u'feature37', u'feature38', u'feature39', u'feature40',\n",
    "           u'feature41', u'feature42', u'feature43', u'feature44', u'feature45',\n",
    "           u'feature46', u'feature47', u'feature48', u'feature49', u'feature50']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is partitioning the training data into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[col_names[\"folds\"]] = np.random.choice(range(1, n_folds + 1), train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train.to_csv(workdir+'numerai_training_data_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the folds, we'll use that one as testing set and the others as training set, storing the predictions of the current base model in new column in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_by_folds(model, model_name, train_df, folds_col, target_col, features_col): \n",
    "    \n",
    "    folds = train_df[folds_col]\n",
    "    target = train_df[target_col]\n",
    "    data = train_df[features_col]\n",
    "    \n",
    "    for f in folds.unique():\n",
    "        train_i = folds != f\n",
    "        train1 = data[train_i]\n",
    "        test1 = data[-train_i]\n",
    "        y1 = target[train_i]\n",
    "        model.fit(train1.as_matrix(), y1.as_matrix().ravel())\n",
    "        y1_pred = model.predict_proba(test1.as_matrix())\n",
    "        train_df.loc[-train_i, model_name] = y1_pred[:,1]\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each base model will be fitted with the full training dataset and used to make predictions on the test set. These predictions will be stored in a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(model, train_df, test_df, target_col, features_col): \n",
    "    X_train = train_df[features_col].as_matrix()\n",
    "    y_train = train_df[target_col].as_matrix().ravel()\n",
    "    X_test = test_df[features_col].as_matrix()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    y_pred = y_pred[:,1]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(workdir+'numerai_tournament_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def models_training_and_prediction(models, train_set, test_set, column_names):\n",
    "    l1_predictions = test_set[[column_names[\"ids\"]]]\n",
    "    train = train_set.copy()\n",
    "    for model in models:\n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        print(\"Training %s ...\" % (model_name))\n",
    "        train = predict_by_folds(model, model_name, train, column_names[\"folds\"], column_names[\"target\"], column_names[\"features\"])\n",
    "        prediction = make_predictions(model, train_set, test_set, column_names[\"target\"], column_names[\"features\"])\n",
    "        l1_predictions.loc[:, (model_name)] = prediction\n",
    "        \n",
    "    return train, l1_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 6 models as level 1 (or base) models. These algorithms were chosen to be significantly different of each other because diversity is desirable while doing stacking. The hyperparameters for each model were individually optimized (in most cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model 1: XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "best_params = {'colsample_bytree': 0.3, 'learning_rate': 0.01, 'min_child_weight': 3.0, \n",
    "               'n_estimators': 400, 'subsample': 0.2, 'max_depth': 5, 'gamma': 0.95}\n",
    "l1_models.append(XGBClassifier(**best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 2: Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "l1_models.append(GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model 3: LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_params = {'max_iter':500, 'C':0.1}\n",
    "l1_models.append(LogisticRegression(**best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 4: Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "best_params = {\n",
    "    \"hidden_layer_sizes\":(100, 100, 50, ), \"activation\":'tanh', \"solver\":'adam', \n",
    "    \"alpha\":0.0001, \"learning_rate\":'invscaling', \"learning_rate_init\":0.001\n",
    "}\n",
    "l1_models.append(MLPClassifier(**best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model 5: RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "best_params = {'min_samples_split': 4, 'n_estimators': 1200}\n",
    "best_params[\"n_jobs\"] =  -1\n",
    "l1_models.append(RandomForestClassifier(**best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 6: k-NN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "best_params = {'n_neighbors': 800}\n",
    "best_params[\"n_jobs\"] = -1\n",
    "l1_models.append(KNeighborsClassifier(**best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBClassifier ...\n",
      "Training GaussianNB ...\n",
      "Training LogisticRegression ...\n",
      "Training MLPClassifier ...\n",
      "Training RandomForestClassifier ...\n",
      "Training KNeighborsClassifier ...\n",
      "CPU times: user 5h 41min 21s, sys: 5min 43s, total: 5h 47min 4s\n",
      "Wall time: 1h 28min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train1, l1_predictions = models_training_and_prediction(l1_models, train, test, col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The level 2 model (or stacking model) will be trained using the predictions of the base models as features. Here we'll use a LogisticRegressor classifier as stacking model because is simple and fast, and allow us to check the fitted coefficients of the level 1 models so we can compare the relative weight of each model in the stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_model_names = [str(m).split(\"(\")[0] for m in l1_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XGBClassifier',\n",
       " 'GaussianNB',\n",
       " 'LogisticRegression',\n",
       " 'MLPClassifier',\n",
       " 'RandomForestClassifier',\n",
       " 'KNeighborsClassifier']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train2 = train1[l1_model_names]\n",
    "target = train[col_names[\"target\"]]\n",
    "test2 = l1_predictions[l1_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=500,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "model2 = LogisticRegressionCV(max_iter=500, n_jobs=-1)\n",
    "model2.fit(train2, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('XGBClassifier', 1.2076147893880145),\n",
       " ('GaussianNB', -0.036017735049057129),\n",
       " ('LogisticRegression', 1.8289543650577398),\n",
       " ('MLPClassifier', 0.29037835977075915),\n",
       " ('RandomForestClassifier', 0.32329146264212677),\n",
       " ('KNeighborsClassifier', 0.85481792947028812)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(l1_model_names, model2.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction2 = model2.predict_proba(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(workdir+\"example_predictions.csv\")\n",
    "results[\"probability\"] = prediction2[:,1]\n",
    "results.to_csv(workdir+\"submission_stacked_lr_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*submission_stacked_lr_1.csv has logloss of 0.689*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling with Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction3 = test2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results[\"probability\"] = prediction3\n",
    "results.to_csv(workdir+\"submission_ensemble_average_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*submission_ensemble_average_1.csv has logloss of 0.687*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes (oftentimes) a simple approach is the best way"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
