{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cervical Cancer Screening.\n",
    "https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening \n",
    "\n",
    "In this competition, you'll work with an image dataset to accurately determine cervix types. Doing so is critical to providing cervical cancer treatments that work for all women regardless of their physiology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN 5110)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. List of training images\n",
    "\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "dataset_path = data_path + \"train/\"\n",
    "train_images = pd.DataFrame(columns=[\"Class\", \"Image\", \"Imagepath\"])\n",
    "for (folder, subs, files) in os.walk(dataset_path):\n",
    "    for filename in files:\n",
    "        label = folder.split(\"/\")[-1]\n",
    "        imagepath = os.path.join(folder, filename)\n",
    "        train_images = train_images.append({\"Class\":label, \n",
    "                                                \"Image\":filename, \n",
    "                                                \"Imagepath\":imagepath}, \n",
    "                                               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Image</th>\n",
       "      <th>Imagepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Type_3</td>\n",
       "      <td>944.jpg</td>\n",
       "      <td>/home/ubuntu/data/train/Type_3/944.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Type_3</td>\n",
       "      <td>427.jpg</td>\n",
       "      <td>/home/ubuntu/data/train/Type_3/427.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Type_3</td>\n",
       "      <td>258.jpg</td>\n",
       "      <td>/home/ubuntu/data/train/Type_3/258.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Type_3</td>\n",
       "      <td>1372.jpg</td>\n",
       "      <td>/home/ubuntu/data/train/Type_3/1372.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Type_3</td>\n",
       "      <td>494.jpg</td>\n",
       "      <td>/home/ubuntu/data/train/Type_3/494.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class     Image                                Imagepath\n",
       "0  Type_3   944.jpg   /home/ubuntu/data/train/Type_3/944.jpg\n",
       "1  Type_3   427.jpg   /home/ubuntu/data/train/Type_3/427.jpg\n",
       "2  Type_3   258.jpg   /home/ubuntu/data/train/Type_3/258.jpg\n",
       "3  Type_3  1372.jpg  /home/ubuntu/data/train/Type_3/1372.jpg\n",
       "4  Type_3   494.jpg   /home/ubuntu/data/train/Type_3/494.jpg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = train_images[train_images[\"Image\"] != \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5be150b410>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bef845f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Class distribution\n",
    "\n",
    "%matplotlib inline\n",
    "train_images[\"Class\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. Optional: Move all the images into the train directory\n",
    "\n",
    "for origin in train_images[\"Imagepath\"]:\n",
    "    l = origin.split(\"/\")\n",
    "    l.pop(-2)\n",
    "    dest = \"/\".join(l)\n",
    "    shutil.move(origin, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. Save train image list\n",
    "\n",
    "train_images.to_csv(data_path+\"train_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#4. List of test images\n",
    "\n",
    "dataset_path = data_path + \"test/\"\n",
    "test_images = pd.DataFrame(columns=[\"Image\", \"Imagepath\"])\n",
    "for (folder, subs, files) in os.walk(dataset_path):\n",
    "    for filename in files:\n",
    "        imagepath = os.path.join(folder, filename)\n",
    "        test_images = test_images.append({\"Image\":filename, \n",
    "                                                \"Imagepath\":imagepath}, \n",
    "                                               ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Imagepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427.jpg</td>\n",
       "      <td>/home/ubuntu/data/test/427.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258.jpg</td>\n",
       "      <td>/home/ubuntu/data/test/258.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376.jpg</td>\n",
       "      <td>/home/ubuntu/data/test/376.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131.jpg</td>\n",
       "      <td>/home/ubuntu/data/test/131.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494.jpg</td>\n",
       "      <td>/home/ubuntu/data/test/494.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Image                       Imagepath\n",
       "0  427.jpg  /home/ubuntu/data/test/427.jpg\n",
       "1  258.jpg  /home/ubuntu/data/test/258.jpg\n",
       "2  376.jpg  /home/ubuntu/data/test/376.jpg\n",
       "3  131.jpg  /home/ubuntu/data/test/131.jpg\n",
       "4  494.jpg  /home/ubuntu/data/test/494.jpg"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_images = test_images[test_images[\"Image\"] != \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#5. Save test image list\n",
    "\n",
    "test_images.to_csv(data_path+\"test_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features with a pretrained deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to load an image as a numpy matrix\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "def get_image_as_X(path_to_image_file, target_size, dim_ordering='th'):\n",
    "    img = load_img(path_to_image_file, grayscale=False, target_size=target_size)\n",
    "    x = img_to_array(img, dim_ordering=dim_ordering)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to extract features from an image with a pretrained model\n",
    "\n",
    "def extract_features(image_list, image_size, model, preprocessing_function):\n",
    "    features = []\n",
    "    processed = []\n",
    "    for image in image_list:\n",
    "        try:\n",
    "            x = get_image_as_X(image, image_size)\n",
    "            x = preprocessing_function(x)\n",
    "            p = model.predict(x)\n",
    "            features.extend(p)\n",
    "            processed.append(image)\n",
    "        except Exception as e:\n",
    "            print(\"Fail with image:\", image)\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    return features, processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Loading the model: ResNet50\n",
    "\n",
    "img_size=(224, 224)\n",
    "from keras.applications import resnet50\n",
    "model = resnet50.ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Image lists\n",
    "train_image_list = train_images[\"Imagepath\"].as_matrix()\n",
    "test_image_list = test_images[\"Imagepath\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1481\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(train_image_list))\n",
    "print(len(test_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail with image: /home/ubuntu/data/train/Type_1/1339.jpg\n",
      "image file is truncated (54 bytes not processed)\n",
      "(1480, 2048, 1, 1)\n",
      "CPU times: user 6min 50s, sys: 1min 8s, total: 7min 59s\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Extracting features of the training images\n",
    "\n",
    "features_resnet50, train_processed = extract_features(train_image_list, img_size, model, resnet50.preprocess_input)\n",
    "features_resnet50_train = np.array(features_resnet50)\n",
    "print(features_resnet50_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fails = [f for f in train_image_list if f not in train_processed]\n",
    "if fails:\n",
    "    train_images = train_images[~train_images[\"Imagepath\"].isin(fails)]\n",
    "    train_images.to_csv(data_path+\"train_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 2048, 1, 1)\n",
      "CPU times: user 2min 24s, sys: 18.2 s, total: 2min 42s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Extracting features of the test images\n",
    "\n",
    "features_resnet50, test_processed = extract_features(test_image_list, img_size, model, resnet50.preprocess_input)\n",
    "features_resnet50_test = np.array(features_resnet50)\n",
    "print(features_resnet50_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fails = [f for f in test_image_list if f not in test_processed]\n",
    "if fails:\n",
    "    test_images = test_images[~test_images[\"Imagepath\"].isin(fails)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving Features\n",
    "np.save(data_path+\"features_resnet50_train.npy\", features_resnet50_train)\n",
    "np.save(data_path+\"features_resnet50_test.npy\", features_resnet50_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1480, 2048)\n",
      "(512, 2048)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(features_resnet50_train, (1480, 2048))\n",
    "X_test = np.reshape(features_resnet50_test, (512, 2048))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_images[\"Class\"].as_matrix()\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 28 ms, total: 1min 49s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = model.fit(X_train, y_train)\n",
    "predictions = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Type_1', 'Type_2', 'Type_3'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def create_submission(path, predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions, columns=['Type_1', 'Type_2', 'Type_3'])\n",
    "    result1.loc[:, 'image_name'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print(\"Writing \" + sub_file)\n",
    "    result1.to_csv(path+sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission_XGB_with_resnet50_feats_2017-03-15-23-38.csv\n"
     ]
    }
   ],
   "source": [
    "create_submission(data_path, predictions, test_images[\"Image\"], \"XGB_with_resnet50_feats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Public Score: 0.88670*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validated_scorer(X_train, y_train, model_class, params, loss, kfolds=3):\n",
    "    params[\"n_estimators\"] = int(params[\"n_estimators\"])\n",
    "    params[\"max_depth\"] = int(params[\"max_depth\"])\n",
    "    print(\"Training with params : %s\" % (params))\n",
    "    mod = model_class(**params)\n",
    "    cv_score = -1 * cross_val_score(mod, X_train, y=y_train, scoring=loss, cv=kfolds, n_jobs=1).mean()\n",
    "    print(cv_score)\n",
    "    return cv_score\n",
    "\n",
    "def optimize(trials):\n",
    "    hyperopt_grid = {\n",
    "            'max_depth' : hp.quniform('max_depth', 1, 10, 1),\n",
    "            'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "            'n_estimators' : hp.quniform('n_estimators', 25, 525, 25),\n",
    "            'gamma' : hp.quniform('gamma', 0.0, 1.0, 0.05),\n",
    "            'min_child_weight' : hp.quniform('min_child_weight', 1, 4, 1),\n",
    "            'subsample' : hp.quniform('subsample', 0.2, 1, 0.1),\n",
    "            'colsample_bytree' : hp.quniform('colsample_bytree', 0.2, 1.0, 0.1)\n",
    "    }\n",
    "    \n",
    "    def objective(params):\n",
    "        err = cross_validated_scorer(X_train, y_train, XGBClassifier, params, loss=\"neg_log_loss\")\n",
    "        return {'loss': err, 'params': params, 'status': STATUS_OK}\n",
    "    best = fmin(objective, hyperopt_grid, algo=tpe.suggest, trials=trials, max_evals=25)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params : {'n_estimators': 400, 'subsample': 0.5, 'colsample_bytree': 0.6000000000000001, 'gamma': 0.8500000000000001, 'learning_rate': 0.21, 'max_depth': 10, 'min_child_weight': 1.0}\n",
      "0.966873901842\n",
      "Training with params : {'n_estimators': 100, 'subsample': 0.5, 'colsample_bytree': 0.30000000000000004, 'gamma': 0.0, 'learning_rate': 0.32, 'max_depth': 5, 'min_child_weight': 3.0}\n",
      "1.0971647344\n",
      "Training with params : {'n_estimators': 325, 'subsample': 0.2, 'colsample_bytree': 0.9, 'gamma': 0.4, 'learning_rate': 0.08, 'max_depth': 5, 'min_child_weight': 3.0}\n",
      "1.00481793258\n",
      "Training with params : {'n_estimators': 475, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.30000000000000004, 'gamma': 0.25, 'learning_rate': 0.08, 'max_depth': 3, 'min_child_weight': 3.0}\n",
      "0.998628793638\n",
      "Training with params : {'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.6000000000000001, 'gamma': 0.45, 'learning_rate': 0.36, 'max_depth': 4, 'min_child_weight': 3.0}\n",
      "1.03067603484\n",
      "Training with params : {'n_estimators': 125, 'subsample': 0.5, 'colsample_bytree': 0.8, 'gamma': 0.9, 'learning_rate': 0.08, 'max_depth': 9, 'min_child_weight': 3.0}\n",
      "0.935639031755\n",
      "Training with params : {'n_estimators': 450, 'subsample': 0.2, 'colsample_bytree': 0.30000000000000004, 'gamma': 0.5, 'learning_rate': 0.15, 'max_depth': 4, 'min_child_weight': 2.0}\n",
      "1.10533753207\n",
      "Training with params : {'n_estimators': 250, 'subsample': 0.5, 'colsample_bytree': 0.9, 'gamma': 0.4, 'learning_rate': 0.34, 'max_depth': 9, 'min_child_weight': 3.0}\n",
      "1.10456043168\n",
      "Training with params : {'n_estimators': 50, 'subsample': 0.8, 'colsample_bytree': 0.6000000000000001, 'gamma': 0.25, 'learning_rate': 0.17, 'max_depth': 7, 'min_child_weight': 3.0}\n",
      "0.954471387783\n",
      "Training with params : {'n_estimators': 225, 'subsample': 0.8, 'colsample_bytree': 0.5, 'gamma': 0.8500000000000001, 'learning_rate': 0.15, 'max_depth': 1, 'min_child_weight': 3.0}\n",
      "0.902648010666\n",
      "Training with params : {'n_estimators': 475, 'subsample': 0.8, 'colsample_bytree': 0.4, 'gamma': 0.45, 'learning_rate': 0.15, 'max_depth': 7, 'min_child_weight': 4.0}\n",
      "0.960994273692\n",
      "Training with params : {'n_estimators': 375, 'subsample': 0.9, 'colsample_bytree': 0.7000000000000001, 'gamma': 0.55, 'learning_rate': 0.16, 'max_depth': 5, 'min_child_weight': 3.0}\n",
      "0.964092654112\n",
      "Training with params : {'n_estimators': 75, 'subsample': 0.4, 'colsample_bytree': 0.8, 'gamma': 0.55, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3.0}\n",
      "0.921160960466\n",
      "Training with params : {'n_estimators': 400, 'subsample': 0.6000000000000001, 'colsample_bytree': 0.6000000000000001, 'gamma': 0.9500000000000001, 'learning_rate': 0.12, 'max_depth': 6, 'min_child_weight': 4.0}\n",
      "0.929873478253\n",
      "Training with params : {'n_estimators': 375, 'subsample': 1.0, 'colsample_bytree': 0.30000000000000004, 'gamma': 0.45, 'learning_rate': 0.43, 'max_depth': 6, 'min_child_weight': 1.0}\n",
      "1.0449332093\n",
      "Training with params : {'n_estimators': 175, 'subsample': 0.6000000000000001, 'colsample_bytree': 0.4, 'gamma': 0.25, 'learning_rate': 0.46, 'max_depth': 9, 'min_child_weight': 3.0}\n",
      "1.169182731\n",
      "Training with params : {'n_estimators': 200, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.9, 'gamma': 0.7000000000000001, 'learning_rate': 0.21, 'max_depth': 10, 'min_child_weight': 3.0}\n",
      "0.980760579385\n",
      "Training with params : {'n_estimators': 225, 'subsample': 0.8, 'colsample_bytree': 0.7000000000000001, 'gamma': 1.0, 'learning_rate': 0.2, 'max_depth': 9, 'min_child_weight': 2.0}\n",
      "0.936356327503\n",
      "Training with params : {'n_estimators': 450, 'subsample': 0.8, 'colsample_bytree': 0.6000000000000001, 'gamma': 0.45, 'learning_rate': 0.09, 'max_depth': 10, 'min_child_weight': 2.0}\n",
      "0.950363794858\n",
      "Training with params : {'n_estimators': 475, 'subsample': 0.4, 'colsample_bytree': 0.7000000000000001, 'gamma': 0.9, 'learning_rate': 0.17, 'max_depth': 3, 'min_child_weight': 1.0}\n",
      "0.987163351108\n",
      "Training with params : {'n_estimators': 50, 'subsample': 0.30000000000000004, 'colsample_bytree': 1.0, 'gamma': 0.75, 'learning_rate': 0.02, 'max_depth': 2, 'min_child_weight': 4.0}\n",
      "0.962650423533\n",
      "Training with params : {'n_estimators': 300, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.5, 'gamma': 0.65, 'learning_rate': 0.01, 'max_depth': 1, 'min_child_weight': 4.0}\n",
      "0.914526580094\n",
      "Training with params : {'n_estimators': 300, 'subsample': 1.0, 'colsample_bytree': 0.5, 'gamma': 0.7000000000000001, 'learning_rate': 0.01, 'max_depth': 1, 'min_child_weight': 4.0}\n",
      "0.921856917853\n",
      "Training with params : {'n_estimators': 275, 'subsample': 0.9, 'colsample_bytree': 0.5, 'gamma': 0.8, 'learning_rate': 0.26, 'max_depth': 1, 'min_child_weight': 4.0}\n",
      "0.964574393138\n",
      "Training with params : {'n_estimators': 150, 'subsample': 0.7000000000000001, 'colsample_bytree': 0.5, 'gamma': 0.6000000000000001, 'learning_rate': 0.04, 'max_depth': 2, 'min_child_weight': 4.0}\n",
      "0.882096308913\n",
      "CPU times: user 2h 24min 10s, sys: 3.91 s, total: 2h 24min 14s\n",
      "Wall time: 18min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trials = Trials()\n",
    "best = optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'learning_rate': 0.04, 'min_child_weight': 4.0, 'n_estimators': 150, 'subsample': 0.7000000000000001, 'max_depth': 2, 'gamma': 0.6000000000000001}\n"
     ]
    }
   ],
   "source": [
    "best[\"n_estimators\"] = int(best[\"n_estimators\"])\n",
    "best[\"max_depth\"] = int(best[\"max_depth\"])\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.7 s, sys: 8 ms, total: 53.7 s\n",
      "Wall time: 6.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = XGBClassifier(**best)\n",
    "model = model.fit(X_train, y_train)\n",
    "predictions = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission_XGB_with_resnet50_feats_opt_2017-03-16-00-16.csv\n"
     ]
    }
   ],
   "source": [
    "create_submission(data_path, predictions, test_images[\"Image\"], \"XGB_with_resnet50_feats_opt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Public Score: 0.86861*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model probability calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "best_model = CalibratedClassifierCV(base_estimator=model, method='sigmoid', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 2s, sys: 184 ms, total: 8min 2s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = best_model.fit(X_train, y_train)\n",
    "predictions = best_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission_XGB_with_resnet50_feats-opt-calibrated_2017-03-16-00-23.csv\n"
     ]
    }
   ],
   "source": [
    "create_submission(data_path, predictions, test_images[\"Image\"], \"XGB_with_resnet50_feats-opt-calibrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Public Score: 0.86595*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
